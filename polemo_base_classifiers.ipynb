{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polemo_base_classifiers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7778c4aeae524e8493daf376a9db1551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b12dea085de74c45a927389a8d8202c7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96d77be7aad44f9686f9b09e7929803d",
              "IPY_MODEL_fcce55f9d58e465fa00f484f75a2ee8c"
            ]
          }
        },
        "b12dea085de74c45a927389a8d8202c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96d77be7aad44f9686f9b09e7929803d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bb4c6549e4d74b27b48aec2b779fd815",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 494801,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 494801,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46381a206d4549bc901f42052e143ba6"
          }
        },
        "fcce55f9d58e465fa00f484f75a2ee8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1623c5846a144b83871086bd33b705fd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 495k/495k [00:01&lt;00:00, 263kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d7d4b5ca2ef4d47982ba97c0ec2c126"
          }
        },
        "bb4c6549e4d74b27b48aec2b779fd815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46381a206d4549bc901f42052e143ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1623c5846a144b83871086bd33b705fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d7d4b5ca2ef4d47982ba97c0ec2c126": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "adc17a3b013a42a5bd90811781b3c645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ced4a3a68bf7448591a44776e6047849",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8a27b57f04d84667b2960b1ad71d404a",
              "IPY_MODEL_42f7701673a84bbea3a54ab0d036a30e"
            ]
          }
        },
        "ced4a3a68bf7448591a44776e6047849": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a27b57f04d84667b2960b1ad71d404a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5eee0b1e67e47eb9b763e3bd2bd7f00",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0902fd633067428ba47760ec6e3d532b"
          }
        },
        "42f7701673a84bbea3a54ab0d036a30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_829245ada0f2417080cc6c91fc448da8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 184B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17fc2cad41274a2caf868c635e3c5c16"
          }
        },
        "a5eee0b1e67e47eb9b763e3bd2bd7f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0902fd633067428ba47760ec6e3d532b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "829245ada0f2417080cc6c91fc448da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17fc2cad41274a2caf868c635e3c5c16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bee85920b7574a37a35ed2921681ed95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_78ba53ebf3c44223b113ab3f757b280a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_907f970f630f4b4397e8a3ddb549648d",
              "IPY_MODEL_27a0918607994221963825d546d94fdf"
            ]
          }
        },
        "78ba53ebf3c44223b113ab3f757b280a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "907f970f630f4b4397e8a3ddb549648d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5c562301648e49b29cc7bbe848014d81",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30ca78cc40734a4687d18c993b41ab9d"
          }
        },
        "27a0918607994221963825d546d94fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db432950026f42a192fa978bff037451",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.00/2.00 [00:00&lt;00:00, 19.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3e6cdae411948078fa8342314800877"
          }
        },
        "5c562301648e49b29cc7bbe848014d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30ca78cc40734a4687d18c993b41ab9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db432950026f42a192fa978bff037451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3e6cdae411948078fa8342314800877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ccfac65a8b3477db6756ebd65b363b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_958511635f1848039fdb97809caebd76",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3f5d739ba7504fcc96dedd68a33e86a6",
              "IPY_MODEL_f2e94160518b4869949c60defefaf982"
            ]
          }
        },
        "958511635f1848039fdb97809caebd76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f5d739ba7504fcc96dedd68a33e86a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6eff535ff3ad41ca8acc98ccc7990452",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 459,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 459,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c62a5fbacc74567bf63c77f94a58ef7"
          }
        },
        "f2e94160518b4869949c60defefaf982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e2a41a2e57f6429197224212f705afa5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 459/459 [00:01&lt;00:00, 327B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c34cd8a8a05d4638b3f82787b8f3b12f"
          }
        },
        "6eff535ff3ad41ca8acc98ccc7990452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c62a5fbacc74567bf63c77f94a58ef7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2a41a2e57f6429197224212f705afa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c34cd8a8a05d4638b3f82787b8f3b12f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "911b30fd0ecc4f89a9ab72491f34dd56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ee1a4329bf7f4bdd811a8bb64ae65ad0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4607ee2d9a5f4ebb9530cbd62a2b9438",
              "IPY_MODEL_7d0df3dcc19d46d8851a5cb6f7405bc2"
            ]
          }
        },
        "ee1a4329bf7f4bdd811a8bb64ae65ad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4607ee2d9a5f4ebb9530cbd62a2b9438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3e7ff4eba8544d2895f113710b4b0859",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 531146902,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 531146902,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_812d823e70074fec9c4985a026b94ed3"
          }
        },
        "7d0df3dcc19d46d8851a5cb6f7405bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e60c9b1239a7438c8f531ce8942a43e0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 531M/531M [00:36&lt;00:00, 14.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d45941419cce408ea090a84e6d9caa32"
          }
        },
        "3e7ff4eba8544d2895f113710b4b0859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "812d823e70074fec9c4985a026b94ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e60c9b1239a7438c8f531ce8942a43e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d45941419cce408ea090a84e6d9caa32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-auWcOy9ClI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f5f31b-0162-4184-ef8b-0acf4eaa59ea"
      },
      "source": [
        "!pip install transformers==2.8.0 -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 573kB 15.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 48.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 53.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 39.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.9MB 48.2MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: botocore 1.19.25 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VObOQ-Lr5zX5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbqxVPA93N4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b8005c-7184-45bd-d1ea-2518b3689304"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#model.cuda()\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcRPH3is5obF"
      },
      "source": [
        "# Dictionary maps task names to required prefixes\n",
        "task_dict = {\n",
        "'MDS-A': [\"all\",\"all\",\"all\"],\n",
        "'SDS-H': [\"hotels\",\"hotels\",\"hotels\"],\n",
        "'SDS-M': [\"medicine\",\"medicine\",\"medicine\"],\n",
        "'SDS-P': [\"products\",\"products\",\"products\"],\n",
        "'SDS-R': [\"reviews\",\"reviews\",\"reviews\"],\n",
        "'DOS-H': [\"shuffled.except.hotels\",\"shuffled.except.hotels\",\"hotels\"],\n",
        "'DOS-M': [\"shuffled.except.medicine\",\"shuffled.except.medicine\",\"medicine\"],\n",
        "'DOS-P': [\"shuffled.except.products\",\"shuffled.except.products\",\"products\"],\n",
        "'DOS-R': [\"shuffled.except.reviews\",\"shuffled.except.reviews\",\"reviews\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOqkY68L4Snw"
      },
      "source": [
        "# Get the data\n",
        "sets = (\"train\", \"dev\", \"test\")\n",
        "\n",
        "def get_sets(task):\n",
        "    return ['%s.sentence.%s.txt'%(task_dict[task][i], d) for i,d in enumerate(sets)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY2z4HqoOOsR"
      },
      "source": [
        "**Hotels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCyGbGUw6OWu"
      },
      "source": [
        "import os\n",
        "\n",
        "TASK = 'SDS-H'\n",
        "\n",
        "for set_ in get_sets(TASK):\n",
        "  os.system('wget https://wothub-data.s3.amazonaws.com/Corpus/%s -nc'%set_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4-69bgd5ryC"
      },
      "source": [
        "train_path, dev_path, test_path = get_sets(TASK)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU5Z7KIF5-Su"
      },
      "source": [
        "# Load data as pandas dataframes with two columns -- sentences and labels\n",
        "train_data = pd.read_csv(train_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "dev_data = pd.read_csv(dev_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "test_data = pd.read_csv(test_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kF_hateWd9R"
      },
      "source": [
        "train_data = train_data[train_data.label != 'z_amb']\n",
        "dev_data = dev_data[dev_data.label != 'z_amb']\n",
        "test_data = test_data[test_data.label != 'z_amb']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZYQIz_dq5aS"
      },
      "source": [
        "# Convert to numpy arrays\n",
        "train_sentences, dev_sentences, test_sentences = [data.iloc[:,0] for data in (train_data, dev_data, test_data)]\n",
        "train_labels, dev_labels, test_labels = [data.iloc[:,1] for data in (train_data, dev_data, test_data)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baomRX_zq4v6"
      },
      "source": [
        "# Check for errors in data labeling, removing nans\n",
        "def remove_nulls(sentences, labels):\n",
        "  lab = pd.Series(labels)\n",
        "  sen = pd.Series(sentences)\n",
        "  lab_nuls = pd.isnull(lab)\n",
        "  sen_nuls = pd.isnull(sen)\n",
        "  not_nuls = ~(lab_nuls | sen_nuls)\n",
        "  lab = lab.loc[not_nuls].to_numpy()\n",
        "  sen = sen.loc[not_nuls].to_numpy()\n",
        "  return [sen, lab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuagUS7KrCqV"
      },
      "source": [
        "train_sentences, train_labels = remove_nulls(train_sentences, train_labels)\n",
        "dev_sentences, dev_labels = remove_nulls(dev_sentences, dev_labels)\n",
        "test_sentences, test_labels = remove_nulls(test_sentences, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQFqttSDM7jA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a756c0-ed28-4beb-e39f-b2f303d9086f"
      },
      "source": [
        "len(np.concatenate((train_labels, dev_labels, test_labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21805"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpKV3JsmMov5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10402235-175c-46cb-a112-08763535f9a8"
      },
      "source": [
        "np.unique(np.concatenate((train_labels, dev_labels, test_labels)), return_counts=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2]), array([10226,  7343,  4236]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubM1xg3XMoUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be97efc5-e226-4f08-d248-b80cca93a9f2"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encoder for labels\n",
        "labelencoder = LabelEncoder()\n",
        "train_labels = labelencoder.fit_transform(train_labels)\n",
        "test_labels = labelencoder.transform(test_labels)\n",
        "dev_labels = labelencoder.transform(dev_labels)\n",
        "labelencoder.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['z_minus_m', 'z_plus_m', 'z_zero'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwgQGgWKm5UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15a11ce-a449-44dc-b528-e45946e2554b"
      },
      "source": [
        "len(train_sentences)  * (1-0.875)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2177.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVTmXmu8Yav3"
      },
      "source": [
        "dev_sentences = np.append(dev_sentences, train_sentences[:2178])\n",
        "dev_labels = np.append(dev_labels, train_labels[:2178])\n",
        "train_sentences = train_sentences[2178:]\n",
        "train_labels = train_labels[2178:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhzo0a06YDM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf4bb72-f8fb-4e16-a2ed-ba9f00e06a49"
      },
      "source": [
        "print('Train data')\n",
        "print(len(train_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Dev data')\n",
        "print(len(dev_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Test data')\n",
        "print(len(test_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data\n",
            "0.6989222655354277\n",
            "Dev data\n",
            "0.20027516624627378\n",
            "Test data\n",
            "0.10080256821829855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGELw2q73kJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7355ad75-87b9-43f4-f96f-ed47c09ae2c2"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-gkcmoz3uBu"
      },
      "source": [
        "dev_polemo_data = pd.DataFrame([dev_sentences,dev_labels]).T\n",
        "test_polemo_data = pd.DataFrame([test_sentences,test_labels]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYq0nR0j30uy"
      },
      "source": [
        "dev_polemo_data.to_csv('/content/drive/My Drive/dev_polemo_hotels_data_preprocessed.csv')\n",
        "test_polemo_data.to_csv('/content/drive/My Drive/test_polemo_hotels_data_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3XrkBeccxdx"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "X = train_sentences\n",
        "y = train_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  train_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  train_labels, test_labels = y[train_index], y[test_index]\n",
        "\n",
        "X = test_sentences\n",
        "y = test_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  dev_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  dev_labels, test_labels = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTCuPf5JrLVs"
      },
      "source": [
        "# Remove long sentences.\n",
        "# TO-DO Possible cut?\n",
        "def remove_big(sentences, labels):\n",
        "  to_remove = []\n",
        "  for i, sent in enumerate(sentences):\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True) # TO-DO: add_special_tokens\n",
        "      if len(input_ids) > MAX_LEN:\n",
        "        to_remove.append(i)\n",
        "\n",
        "  sentences = np.delete(sentences, to_remove)\n",
        "  labels = np.delete(labels, to_remove) \n",
        "\n",
        "  print('{} samples removed.'.format(len(to_remove)))\n",
        "\n",
        "  return sentences, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiEiusol86Ez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "7778c4aeae524e8493daf376a9db1551",
            "b12dea085de74c45a927389a8d8202c7",
            "96d77be7aad44f9686f9b09e7929803d",
            "fcce55f9d58e465fa00f484f75a2ee8c",
            "bb4c6549e4d74b27b48aec2b779fd815",
            "46381a206d4549bc901f42052e143ba6",
            "1623c5846a144b83871086bd33b705fd",
            "4d7d4b5ca2ef4d47982ba97c0ec2c126",
            "adc17a3b013a42a5bd90811781b3c645",
            "ced4a3a68bf7448591a44776e6047849",
            "8a27b57f04d84667b2960b1ad71d404a",
            "42f7701673a84bbea3a54ab0d036a30e",
            "a5eee0b1e67e47eb9b763e3bd2bd7f00",
            "0902fd633067428ba47760ec6e3d532b",
            "829245ada0f2417080cc6c91fc448da8",
            "17fc2cad41274a2caf868c635e3c5c16",
            "bee85920b7574a37a35ed2921681ed95",
            "78ba53ebf3c44223b113ab3f757b280a",
            "907f970f630f4b4397e8a3ddb549648d",
            "27a0918607994221963825d546d94fdf",
            "5c562301648e49b29cc7bbe848014d81",
            "30ca78cc40734a4687d18c993b41ab9d",
            "db432950026f42a192fa978bff037451",
            "f3e6cdae411948078fa8342314800877"
          ]
        },
        "outputId": "e5190e7c-9386-41a1-8a6e-a804fde300ff"
      },
      "source": [
        "# Downloading tokenizer\n",
        "# From Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7778c4aeae524e8493daf376a9db1551",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=494801.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adc17a3b013a42a5bd90811781b3c645",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bee85920b7574a37a35ed2921681ed95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-6Q0ZTGrk0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8407af9a-cbc8-4384-bb24-3efda74b50f5"
      },
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "train_sentences, train_labels = remove_big(train_sentences, train_labels)\n",
        "test_sentences, test_labels = remove_big(test_sentences, test_labels)\n",
        "dev_sentences, dev_labels = remove_big(dev_sentences, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25 samples removed.\n",
            "6 samples removed.\n",
            "3 samples removed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z3HJte1ruLr"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "# Create TensorDatasets for train/dev/test sets\n",
        "def tensor_dataset(sentences, labels):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in sentences:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                     \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = MAX_LEN,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gowQfc1-ryC0"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataset = tensor_dataset(train_sentences, train_labels)\n",
        "test_dataset = tensor_dataset(test_sentences, test_labels)\n",
        "dev_dataset = tensor_dataset(dev_sentences, dev_labels)\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "# Create the DataLoaders for train/dev/test sets.\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = BATCH_SIZE)\n",
        "validation_dataloader = DataLoader(dev_dataset, sampler = SequentialSampler(dev_dataset), batch_size = BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6wLat19-Fdo"
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zjXD2hB30L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2ccfac65a8b3477db6756ebd65b363b2",
            "958511635f1848039fdb97809caebd76",
            "3f5d739ba7504fcc96dedd68a33e86a6",
            "f2e94160518b4869949c60defefaf982",
            "6eff535ff3ad41ca8acc98ccc7990452",
            "4c62a5fbacc74567bf63c77f94a58ef7",
            "e2a41a2e57f6429197224212f705afa5",
            "c34cd8a8a05d4638b3f82787b8f3b12f",
            "911b30fd0ecc4f89a9ab72491f34dd56",
            "ee1a4329bf7f4bdd811a8bb64ae65ad0",
            "4607ee2d9a5f4ebb9530cbd62a2b9438",
            "7d0df3dcc19d46d8851a5cb6f7405bc2",
            "3e7ff4eba8544d2895f113710b4b0859",
            "812d823e70074fec9c4985a026b94ed3",
            "e60c9b1239a7438c8f531ce8942a43e0",
            "d45941419cce408ea090a84e6d9caa32"
          ]
        },
        "outputId": "1910a4ec-507d-44c1-e38e-1f9b4715b87c"
      },
      "source": [
        "# Load model with a sequence classification head\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dkleczek/bert-base-polish-uncased-v1\", # Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ccfac65a8b3477db6756ebd65b363b2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=459.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "911b30fd0ecc4f89a9ab72491f34dd56",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=531146902.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(60000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nrCyZYc_0yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684eba01-ffe3-4af6-a74e-aa8c8d1f3dd8"
      },
      "source": [
        "import time, datetime\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.optimization import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Takes a time in seconds and returns a string hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Parameters:\n",
        "epochs = 3\n",
        "#lr = 1e-3 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "lr = 5e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "adam_epsilon = 1e-8\n",
        "WARM_UP = 0\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = adam_epsilon)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = WARM_UP, num_training_steps = total_steps)\n",
        "\n",
        "train_loss_values = []\n",
        "dev_acc_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch_i in range(0, epochs):  \n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "  print('Training...')\n",
        "\n",
        "  # https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\n",
        "  # linie 168-183\n",
        "  epoch_train_loss = 0 # Cumulative loss\n",
        "  loss = 0 ;     batch_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Progress update every 40 batches.\n",
        "    if step % 40 == 0 and not step == 0:\n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)      \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}. Loss: {:.3f}  Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "    \n",
        "\n",
        "    batch_loss = 0\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)    \n",
        "\n",
        "    # clear any previously calculated gradients before backward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    epoch_train_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Update learning rate schedule\n",
        "\n",
        "  epoch_train_loss = epoch_train_loss / len(train_dataloader)          \n",
        "  train_loss_values.append(epoch_train_loss)\n",
        "  \n",
        "  print('Average training loss: {0:.2f}'.format(epoch_train_loss))\n",
        "\n",
        "  # Evaluation\n",
        "  total_eval_accuracy = 0\n",
        "  model.eval()\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    \n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2].to('cpu').numpy()\n",
        "                \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predictions = np.argmax(logits, axis=1).flatten()\n",
        "    total_eval_accuracy += flat_accuracy(logits, labels)\n",
        "\n",
        "  avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "  print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of  1,521. Loss: 1.166  Elapsed: 0:00:09.\n",
            "  Batch    80  of  1,521. Loss: 0.430  Elapsed: 0:00:17.\n",
            "  Batch   120  of  1,521. Loss: 0.412  Elapsed: 0:00:26.\n",
            "  Batch   160  of  1,521. Loss: 0.635  Elapsed: 0:00:35.\n",
            "  Batch   200  of  1,521. Loss: 1.897  Elapsed: 0:00:43.\n",
            "  Batch   240  of  1,521. Loss: 0.429  Elapsed: 0:00:52.\n",
            "  Batch   280  of  1,521. Loss: 0.347  Elapsed: 0:01:01.\n",
            "  Batch   320  of  1,521. Loss: 0.097  Elapsed: 0:01:11.\n",
            "  Batch   360  of  1,521. Loss: 1.731  Elapsed: 0:01:20.\n",
            "  Batch   400  of  1,521. Loss: 0.180  Elapsed: 0:01:29.\n",
            "  Batch   440  of  1,521. Loss: 0.970  Elapsed: 0:01:38.\n",
            "  Batch   480  of  1,521. Loss: 0.151  Elapsed: 0:01:48.\n",
            "  Batch   520  of  1,521. Loss: 0.106  Elapsed: 0:01:57.\n",
            "  Batch   560  of  1,521. Loss: 0.286  Elapsed: 0:02:06.\n",
            "  Batch   600  of  1,521. Loss: 0.551  Elapsed: 0:02:15.\n",
            "  Batch   640  of  1,521. Loss: 1.340  Elapsed: 0:02:24.\n",
            "  Batch   680  of  1,521. Loss: 0.446  Elapsed: 0:02:33.\n",
            "  Batch   720  of  1,521. Loss: 1.075  Elapsed: 0:02:42.\n",
            "  Batch   760  of  1,521. Loss: 0.012  Elapsed: 0:02:51.\n",
            "  Batch   800  of  1,521. Loss: 1.373  Elapsed: 0:03:01.\n",
            "  Batch   840  of  1,521. Loss: 0.357  Elapsed: 0:03:10.\n",
            "  Batch   880  of  1,521. Loss: 1.105  Elapsed: 0:03:19.\n",
            "  Batch   920  of  1,521. Loss: 0.455  Elapsed: 0:03:28.\n",
            "  Batch   960  of  1,521. Loss: 0.112  Elapsed: 0:03:37.\n",
            "  Batch 1,000  of  1,521. Loss: 0.026  Elapsed: 0:03:46.\n",
            "  Batch 1,040  of  1,521. Loss: 0.789  Elapsed: 0:03:56.\n",
            "  Batch 1,080  of  1,521. Loss: 0.516  Elapsed: 0:04:05.\n",
            "  Batch 1,120  of  1,521. Loss: 0.230  Elapsed: 0:04:14.\n",
            "  Batch 1,160  of  1,521. Loss: 0.418  Elapsed: 0:04:23.\n",
            "  Batch 1,200  of  1,521. Loss: 0.116  Elapsed: 0:04:32.\n",
            "  Batch 1,240  of  1,521. Loss: 0.349  Elapsed: 0:04:41.\n",
            "  Batch 1,280  of  1,521. Loss: 0.631  Elapsed: 0:04:51.\n",
            "  Batch 1,320  of  1,521. Loss: 0.632  Elapsed: 0:05:00.\n",
            "  Batch 1,360  of  1,521. Loss: 0.999  Elapsed: 0:05:09.\n",
            "  Batch 1,400  of  1,521. Loss: 1.743  Elapsed: 0:05:18.\n",
            "  Batch 1,440  of  1,521. Loss: 0.011  Elapsed: 0:05:27.\n",
            "  Batch 1,480  of  1,521. Loss: 0.076  Elapsed: 0:05:36.\n",
            "  Batch 1,520  of  1,521. Loss: 0.029  Elapsed: 0:05:46.\n",
            "Average training loss: 0.49\n",
            "  Accuracy: 0.8599\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  1,521. Loss: 0.003  Elapsed: 0:06:09.\n",
            "  Batch    80  of  1,521. Loss: 0.004  Elapsed: 0:06:18.\n",
            "  Batch   120  of  1,521. Loss: 0.510  Elapsed: 0:06:27.\n",
            "  Batch   160  of  1,521. Loss: 0.494  Elapsed: 0:06:36.\n",
            "  Batch   200  of  1,521. Loss: 0.008  Elapsed: 0:06:45.\n",
            "  Batch   240  of  1,521. Loss: 0.003  Elapsed: 0:06:54.\n",
            "  Batch   280  of  1,521. Loss: 0.240  Elapsed: 0:07:03.\n",
            "  Batch   320  of  1,521. Loss: 0.004  Elapsed: 0:07:12.\n",
            "  Batch   360  of  1,521. Loss: 0.006  Elapsed: 0:07:22.\n",
            "  Batch   400  of  1,521. Loss: 0.356  Elapsed: 0:07:31.\n",
            "  Batch   440  of  1,521. Loss: 0.136  Elapsed: 0:07:40.\n",
            "  Batch   480  of  1,521. Loss: 0.876  Elapsed: 0:07:49.\n",
            "  Batch   520  of  1,521. Loss: 0.007  Elapsed: 0:07:58.\n",
            "  Batch   560  of  1,521. Loss: 0.003  Elapsed: 0:08:07.\n",
            "  Batch   600  of  1,521. Loss: 1.166  Elapsed: 0:08:16.\n",
            "  Batch   640  of  1,521. Loss: 0.093  Elapsed: 0:08:25.\n",
            "  Batch   680  of  1,521. Loss: 1.491  Elapsed: 0:08:34.\n",
            "  Batch   720  of  1,521. Loss: 1.706  Elapsed: 0:08:44.\n",
            "  Batch   760  of  1,521. Loss: 0.016  Elapsed: 0:08:53.\n",
            "  Batch   800  of  1,521. Loss: 0.662  Elapsed: 0:09:02.\n",
            "  Batch   840  of  1,521. Loss: 0.504  Elapsed: 0:09:11.\n",
            "  Batch   880  of  1,521. Loss: 0.001  Elapsed: 0:09:20.\n",
            "  Batch   920  of  1,521. Loss: 0.010  Elapsed: 0:09:29.\n",
            "  Batch   960  of  1,521. Loss: 0.209  Elapsed: 0:09:38.\n",
            "  Batch 1,000  of  1,521. Loss: 1.008  Elapsed: 0:09:47.\n",
            "  Batch 1,040  of  1,521. Loss: 0.140  Elapsed: 0:09:56.\n",
            "  Batch 1,080  of  1,521. Loss: 0.788  Elapsed: 0:10:05.\n",
            "  Batch 1,120  of  1,521. Loss: 1.013  Elapsed: 0:10:15.\n",
            "  Batch 1,160  of  1,521. Loss: 0.750  Elapsed: 0:10:24.\n",
            "  Batch 1,200  of  1,521. Loss: 0.007  Elapsed: 0:10:33.\n",
            "  Batch 1,240  of  1,521. Loss: 0.002  Elapsed: 0:10:42.\n",
            "  Batch 1,280  of  1,521. Loss: 0.004  Elapsed: 0:10:51.\n",
            "  Batch 1,320  of  1,521. Loss: 0.002  Elapsed: 0:11:00.\n",
            "  Batch 1,360  of  1,521. Loss: 0.006  Elapsed: 0:11:09.\n",
            "  Batch 1,400  of  1,521. Loss: 1.648  Elapsed: 0:11:18.\n",
            "  Batch 1,440  of  1,521. Loss: 0.775  Elapsed: 0:11:27.\n",
            "  Batch 1,480  of  1,521. Loss: 0.257  Elapsed: 0:11:36.\n",
            "  Batch 1,520  of  1,521. Loss: 0.003  Elapsed: 0:11:45.\n",
            "Average training loss: 0.24\n",
            "  Accuracy: 0.8868\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  1,521. Loss: 0.001  Elapsed: 0:12:08.\n",
            "  Batch    80  of  1,521. Loss: 0.276  Elapsed: 0:12:17.\n",
            "  Batch   120  of  1,521. Loss: 1.027  Elapsed: 0:12:26.\n",
            "  Batch   160  of  1,521. Loss: 0.002  Elapsed: 0:12:35.\n",
            "  Batch   200  of  1,521. Loss: 0.001  Elapsed: 0:12:44.\n",
            "  Batch   240  of  1,521. Loss: 0.001  Elapsed: 0:12:53.\n",
            "  Batch   280  of  1,521. Loss: 0.001  Elapsed: 0:13:02.\n",
            "  Batch   320  of  1,521. Loss: 0.001  Elapsed: 0:13:11.\n",
            "  Batch   360  of  1,521. Loss: 0.002  Elapsed: 0:13:20.\n",
            "  Batch   400  of  1,521. Loss: 0.001  Elapsed: 0:13:29.\n",
            "  Batch   440  of  1,521. Loss: 0.174  Elapsed: 0:13:38.\n",
            "  Batch   480  of  1,521. Loss: 0.000  Elapsed: 0:13:47.\n",
            "  Batch   520  of  1,521. Loss: 0.000  Elapsed: 0:13:56.\n",
            "  Batch   560  of  1,521. Loss: 0.001  Elapsed: 0:14:05.\n",
            "  Batch   600  of  1,521. Loss: 0.016  Elapsed: 0:14:14.\n",
            "  Batch   640  of  1,521. Loss: 0.001  Elapsed: 0:14:23.\n",
            "  Batch   680  of  1,521. Loss: 0.002  Elapsed: 0:14:32.\n",
            "  Batch   720  of  1,521. Loss: 0.000  Elapsed: 0:14:41.\n",
            "  Batch   760  of  1,521. Loss: 0.228  Elapsed: 0:14:50.\n",
            "  Batch   800  of  1,521. Loss: 0.001  Elapsed: 0:14:59.\n",
            "  Batch   840  of  1,521. Loss: 0.161  Elapsed: 0:15:08.\n",
            "  Batch   880  of  1,521. Loss: 0.001  Elapsed: 0:15:17.\n",
            "  Batch   920  of  1,521. Loss: 0.043  Elapsed: 0:15:26.\n",
            "  Batch   960  of  1,521. Loss: 0.001  Elapsed: 0:15:35.\n",
            "  Batch 1,000  of  1,521. Loss: 0.001  Elapsed: 0:15:44.\n",
            "  Batch 1,040  of  1,521. Loss: 0.000  Elapsed: 0:15:53.\n",
            "  Batch 1,080  of  1,521. Loss: 0.000  Elapsed: 0:16:02.\n",
            "  Batch 1,120  of  1,521. Loss: 0.001  Elapsed: 0:16:11.\n",
            "  Batch 1,160  of  1,521. Loss: 0.001  Elapsed: 0:16:20.\n",
            "  Batch 1,200  of  1,521. Loss: 0.432  Elapsed: 0:16:29.\n",
            "  Batch 1,240  of  1,521. Loss: 0.002  Elapsed: 0:16:38.\n",
            "  Batch 1,280  of  1,521. Loss: 0.001  Elapsed: 0:16:47.\n",
            "  Batch 1,320  of  1,521. Loss: 0.498  Elapsed: 0:16:56.\n",
            "  Batch 1,360  of  1,521. Loss: 0.001  Elapsed: 0:17:05.\n",
            "  Batch 1,400  of  1,521. Loss: 0.000  Elapsed: 0:17:14.\n",
            "  Batch 1,440  of  1,521. Loss: 0.001  Elapsed: 0:17:23.\n",
            "  Batch 1,480  of  1,521. Loss: 0.001  Elapsed: 0:17:32.\n",
            "  Batch 1,520  of  1,521. Loss: 0.950  Elapsed: 0:17:41.\n",
            "Average training loss: 0.09\n",
            "  Accuracy: 0.8776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJmF205y1A8G"
      },
      "source": [
        "predicted_labels = [] ; true_labels = []; logits_list = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  \n",
        "  input_ids = batch[0].to(device)\n",
        "  attention_masks = batch[1].to(device)\n",
        "  labels = batch[2]\n",
        "  \n",
        "  with torch.no_grad():        \n",
        "      outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "                  \n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  logits_list.append(logits)\n",
        "  \n",
        "  predictions = np.argmax(logits, axis=1).flatten()\n",
        "  labels = labels.numpy().flatten()\n",
        "\n",
        "  predicted_labels.extend( predictions )\n",
        "  true_labels.extend( labels )\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRjDx1CUYXaY"
      },
      "source": [
        "def inverse_logit(x):\n",
        "  return np.exp(x) / (1 + np.exp(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzgRqk3l1Mnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f10776-5a7f-48e7-fac4-220dd9acfabd"
      },
      "source": [
        "# Parameters:\n",
        "#epochs = 2\n",
        "#lr = 3e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "#adam_epsilon = 1e-8\n",
        "#WARM_UP = 0\n",
        "#87 86 87\n",
        "from sklearn.metrics import classification_report \n",
        "print( classification_report(y_true=true_labels, y_pred=predicted_labels, zero_division=0) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92       706\n",
            "           1       0.89      0.92      0.91       514\n",
            "           2       0.84      0.76      0.80       298\n",
            "\n",
            "    accuracy                           0.89      1518\n",
            "   macro avg       0.88      0.87      0.88      1518\n",
            "weighted avg       0.89      0.89      0.89      1518\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNyHp809jvfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44975b82-5f0a-4358-a756-8c69a7537611"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNpNPjA4i1ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6d8215-910d-4fa0-c714-c4573fd3dc24"
      },
      "source": [
        "import os\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "output_dir = \"/content/drive/My Drive/model_bert_finetuned_1_1\"\n",
        "\n",
        "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "# If we have a distributed model, save only the encapsulated model\n",
        "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/model_bert_finetuned_1_1/vocab.txt',\n",
              " '/content/drive/My Drive/model_bert_finetuned_1_1/special_tokens_map.json',\n",
              " '/content/drive/My Drive/model_bert_finetuned_1_1/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT13bT9EkRUX"
      },
      "source": [
        "# Step 2: Re-load the saved model and vocabulary\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ru-mk2ATSla"
      },
      "source": [
        "**Medicine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBssHqemt2mC"
      },
      "source": [
        "import os\n",
        "\n",
        "TASK = 'SDS-M'\n",
        "\n",
        "for set_ in get_sets(TASK):\n",
        "  os.system('wget https://wothub-data.s3.amazonaws.com/Corpus/%s -nc'%set_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSCSkEdNTjab"
      },
      "source": [
        "train_path, dev_path, test_path = get_sets(TASK)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSBPcX19TjtV"
      },
      "source": [
        "# Load data as pandas dataframes with two columns -- sentences and labels\n",
        "train_data = pd.read_csv(train_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "dev_data = pd.read_csv(dev_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "test_data = pd.read_csv(test_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SWjhJg5Tj8Q"
      },
      "source": [
        "train_data = train_data[train_data.label != 'z_amb']\n",
        "dev_data = dev_data[dev_data.label != 'z_amb']\n",
        "test_data = test_data[test_data.label != 'z_amb']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oac7dmxkTryJ"
      },
      "source": [
        "# Convert to numpy arrays\n",
        "train_sentences, dev_sentences, test_sentences = [data.iloc[:,0] for data in (train_data, dev_data, test_data)]\n",
        "train_labels, dev_labels, test_labels = [data.iloc[:,1] for data in (train_data, dev_data, test_data)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Oqy3-3Tv83"
      },
      "source": [
        "# Check for errors in data labeling, removing nans\n",
        "def remove_nulls(sentences, labels):\n",
        "  lab = pd.Series(labels)\n",
        "  sen = pd.Series(sentences)\n",
        "  lab_nuls = pd.isnull(lab)\n",
        "  sen_nuls = pd.isnull(sen)\n",
        "  not_nuls = ~(lab_nuls | sen_nuls)\n",
        "  lab = lab.loc[not_nuls].to_numpy()\n",
        "  sen = sen.loc[not_nuls].to_numpy()\n",
        "  return [sen, lab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBkfx3B6TzL_"
      },
      "source": [
        "train_sentences, train_labels = remove_nulls(train_sentences, train_labels)\n",
        "dev_sentences, dev_labels = remove_nulls(dev_sentences, dev_labels)\n",
        "test_sentences, test_labels = remove_nulls(test_sentences, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLb2pdt3T2pI",
        "outputId": "5d2f8685-fb92-42f8-d63b-98ca529bc5cb"
      },
      "source": [
        "np.unique(np.concatenate((train_labels, dev_labels, test_labels)), return_counts=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2]), array([7041, 5253, 8945]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdVvSVDYT5MX",
        "outputId": "70ea3f43-77c6-45c3-be4c-527c4495645c"
      },
      "source": [
        "len(train_sentences)  * (1-0.875)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2126.375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2QyLPgztQOO",
        "outputId": "08d44e94-b5bd-4159-d02c-a6319bdae7fd"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encoder for labels\n",
        "labelencoder = LabelEncoder()\n",
        "train_labels = labelencoder.fit_transform(train_labels)\n",
        "test_labels = labelencoder.transform(test_labels)\n",
        "dev_labels = labelencoder.transform(dev_labels)\n",
        "labelencoder.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['z_minus_m', 'z_plus_m', 'z_zero'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHkPcS87T8sX"
      },
      "source": [
        "dev_sentences = np.append(dev_sentences, train_sentences[:2127])\n",
        "dev_labels = np.append(dev_labels, train_labels[:2127])\n",
        "train_sentences = train_sentences[2127:]\n",
        "train_labels = train_labels[2127:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEXdxKMST_an",
        "outputId": "0d712252-ada9-4f2a-c812-eb6644da504e"
      },
      "source": [
        "print('Train data')\n",
        "print(len(train_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Dev data')\n",
        "print(len(dev_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Test data')\n",
        "print(len(test_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data\n",
            "0.7007862893733227\n",
            "Dev data\n",
            "0.19977400065916476\n",
            "Test data\n",
            "0.0994397099675126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzOPFqERUCtW"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybd_2eadUFGR"
      },
      "source": [
        "dev_polemo_data = pd.DataFrame([dev_sentences,dev_labels]).T\n",
        "test_polemo_data = pd.DataFrame([test_sentences,test_labels]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZwK7_IQUHiN"
      },
      "source": [
        "dev_polemo_data.to_csv('/content/drive/My Drive/dev_polemo_medicine_data_preprocessed.csv')\n",
        "test_polemo_data.to_csv('/content/drive/My Drive/test_polemo_medicine_data_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6-VKPZYUKRF"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "X = train_sentences\n",
        "y = train_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  train_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  train_labels, test_labels = y[train_index], y[test_index]\n",
        "\n",
        "X = test_sentences\n",
        "y = test_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  dev_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  dev_labels, test_labels = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "refWcj9uUNS5"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encoder for labels\n",
        "labelencoder = LabelEncoder()\n",
        "train_labels = labelencoder.fit_transform(train_labels)\n",
        "test_labels = labelencoder.transform(test_labels)\n",
        "dev_labels = labelencoder.transform(dev_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPjljL0MUPxY"
      },
      "source": [
        "# Remove long sentences.\n",
        "# TO-DO Possible cut?\n",
        "def remove_big(sentences, labels):\n",
        "  to_remove = []\n",
        "  for i, sent in enumerate(sentences):\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True) # TO-DO: add_special_tokens\n",
        "      if len(input_ids) > MAX_LEN:\n",
        "        to_remove.append(i)\n",
        "\n",
        "  sentences = np.delete(sentences, to_remove)\n",
        "  labels = np.delete(labels, to_remove) \n",
        "\n",
        "  print('{} samples removed.'.format(len(to_remove)))\n",
        "\n",
        "  return sentences, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4xFwxnKUSJM"
      },
      "source": [
        "# Downloading tokenizer\n",
        "# From Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGD-T-75UU3R",
        "outputId": "5273af1e-84f0-4ca5-d50e-f7004e419633"
      },
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "train_sentences, train_labels = remove_big(train_sentences, train_labels)\n",
        "test_sentences, test_labels = remove_big(test_sentences, test_labels)\n",
        "dev_sentences, dev_labels = remove_big(dev_sentences, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14 samples removed.\n",
            "3 samples removed.\n",
            "0 samples removed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdKtxZp0UYt_"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "# Create TensorDatasets for train/dev/test sets\n",
        "def tensor_dataset(sentences, labels):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in sentences:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                     \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = MAX_LEN,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt7t7QViUbuz"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataset = tensor_dataset(train_sentences, train_labels)\n",
        "test_dataset = tensor_dataset(test_sentences, test_labels)\n",
        "dev_dataset = tensor_dataset(dev_sentences, dev_labels)\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "# Create the DataLoaders for train/dev/test sets.\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = BATCH_SIZE)\n",
        "validation_dataloader = DataLoader(dev_dataset, sampler = SequentialSampler(dev_dataset), batch_size = BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty8-kicqUffu",
        "outputId": "5f84e3d8-6cab-48d2-ee5c-b9d301c60c83"
      },
      "source": [
        "# Load model with a sequence classification head\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dkleczek/bert-base-polish-uncased-v1\", # Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(60000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gJhjMf4Ui6G",
        "outputId": "9c69a35e-452b-4aec-dadf-a69a0842ad20"
      },
      "source": [
        "import time, datetime\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.optimization import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Takes a time in seconds and returns a string hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Parameters:\n",
        "epochs = 3\n",
        "#lr = 1e-3 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "lr = 5e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "adam_epsilon = 1e-8\n",
        "WARM_UP = 0\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = adam_epsilon)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = WARM_UP, num_training_steps = total_steps)\n",
        "\n",
        "train_loss_values = []\n",
        "dev_acc_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch_i in range(0, epochs):  \n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "  print('Training...')\n",
        "\n",
        "  # https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\n",
        "  # linie 168-183\n",
        "  epoch_train_loss = 0 # Cumulative loss\n",
        "  loss = 0 ;     batch_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Progress update every 40 batches.\n",
        "    if step % 40 == 0 and not step == 0:\n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)      \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}. Loss: {:.3f}  Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "    \n",
        "\n",
        "    batch_loss = 0\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)    \n",
        "\n",
        "    # clear any previously calculated gradients before backward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    epoch_train_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Update learning rate schedule\n",
        "\n",
        "  epoch_train_loss = epoch_train_loss / len(train_dataloader)          \n",
        "  train_loss_values.append(epoch_train_loss)\n",
        "  \n",
        "  print('Average training loss: {0:.2f}'.format(epoch_train_loss))\n",
        "\n",
        "  # Evaluation\n",
        "  total_eval_accuracy = 0\n",
        "  model.eval()\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    \n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2].to('cpu').numpy()\n",
        "                \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predictions = np.argmax(logits, axis=1).flatten()\n",
        "    total_eval_accuracy += flat_accuracy(logits, labels)\n",
        "\n",
        "  avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "  print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of  1,487. Loss: 0.950  Elapsed: 0:00:09.\n",
            "  Batch    80  of  1,487. Loss: 1.166  Elapsed: 0:00:18.\n",
            "  Batch   120  of  1,487. Loss: 0.445  Elapsed: 0:00:27.\n",
            "  Batch   160  of  1,487. Loss: 1.185  Elapsed: 0:00:36.\n",
            "  Batch   200  of  1,487. Loss: 0.564  Elapsed: 0:00:46.\n",
            "  Batch   240  of  1,487. Loss: 1.034  Elapsed: 0:00:55.\n",
            "  Batch   280  of  1,487. Loss: 0.623  Elapsed: 0:01:04.\n",
            "  Batch   320  of  1,487. Loss: 0.258  Elapsed: 0:01:13.\n",
            "  Batch   360  of  1,487. Loss: 0.572  Elapsed: 0:01:22.\n",
            "  Batch   400  of  1,487. Loss: 1.653  Elapsed: 0:01:31.\n",
            "  Batch   440  of  1,487. Loss: 1.127  Elapsed: 0:01:41.\n",
            "  Batch   480  of  1,487. Loss: 0.700  Elapsed: 0:01:50.\n",
            "  Batch   520  of  1,487. Loss: 0.484  Elapsed: 0:01:59.\n",
            "  Batch   560  of  1,487. Loss: 0.587  Elapsed: 0:02:08.\n",
            "  Batch   600  of  1,487. Loss: 0.517  Elapsed: 0:02:17.\n",
            "  Batch   640  of  1,487. Loss: 0.639  Elapsed: 0:02:27.\n",
            "  Batch   680  of  1,487. Loss: 0.878  Elapsed: 0:02:36.\n",
            "  Batch   720  of  1,487. Loss: 1.462  Elapsed: 0:02:45.\n",
            "  Batch   760  of  1,487. Loss: 0.469  Elapsed: 0:02:54.\n",
            "  Batch   800  of  1,487. Loss: 0.383  Elapsed: 0:03:03.\n",
            "  Batch   840  of  1,487. Loss: 0.668  Elapsed: 0:03:13.\n",
            "  Batch   880  of  1,487. Loss: 1.037  Elapsed: 0:03:22.\n",
            "  Batch   920  of  1,487. Loss: 0.615  Elapsed: 0:03:31.\n",
            "  Batch   960  of  1,487. Loss: 0.241  Elapsed: 0:03:40.\n",
            "  Batch 1,000  of  1,487. Loss: 1.332  Elapsed: 0:03:49.\n",
            "  Batch 1,040  of  1,487. Loss: 1.030  Elapsed: 0:03:58.\n",
            "  Batch 1,080  of  1,487. Loss: 0.922  Elapsed: 0:04:08.\n",
            "  Batch 1,120  of  1,487. Loss: 0.867  Elapsed: 0:04:17.\n",
            "  Batch 1,160  of  1,487. Loss: 0.645  Elapsed: 0:04:26.\n",
            "  Batch 1,200  of  1,487. Loss: 0.720  Elapsed: 0:04:35.\n",
            "  Batch 1,240  of  1,487. Loss: 0.448  Elapsed: 0:04:44.\n",
            "  Batch 1,280  of  1,487. Loss: 0.680  Elapsed: 0:04:53.\n",
            "  Batch 1,320  of  1,487. Loss: 0.450  Elapsed: 0:05:02.\n",
            "  Batch 1,360  of  1,487. Loss: 1.511  Elapsed: 0:05:12.\n",
            "  Batch 1,400  of  1,487. Loss: 0.293  Elapsed: 0:05:21.\n",
            "  Batch 1,440  of  1,487. Loss: 0.686  Elapsed: 0:05:30.\n",
            "  Batch 1,480  of  1,487. Loss: 0.087  Elapsed: 0:05:39.\n",
            "Average training loss: 0.56\n",
            "  Accuracy: 0.8212\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  1,487. Loss: 0.060  Elapsed: 0:06:03.\n",
            "  Batch    80  of  1,487. Loss: 0.006  Elapsed: 0:06:12.\n",
            "  Batch   120  of  1,487. Loss: 0.652  Elapsed: 0:06:21.\n",
            "  Batch   160  of  1,487. Loss: 0.615  Elapsed: 0:06:31.\n",
            "  Batch   200  of  1,487. Loss: 0.004  Elapsed: 0:06:40.\n",
            "  Batch   240  of  1,487. Loss: 0.619  Elapsed: 0:06:49.\n",
            "  Batch   280  of  1,487. Loss: 0.888  Elapsed: 0:06:58.\n",
            "  Batch   320  of  1,487. Loss: 0.005  Elapsed: 0:07:07.\n",
            "  Batch   360  of  1,487. Loss: 0.233  Elapsed: 0:07:16.\n",
            "  Batch   400  of  1,487. Loss: 0.001  Elapsed: 0:07:25.\n",
            "  Batch   440  of  1,487. Loss: 0.293  Elapsed: 0:07:34.\n",
            "  Batch   480  of  1,487. Loss: 0.041  Elapsed: 0:07:43.\n",
            "  Batch   520  of  1,487. Loss: 0.864  Elapsed: 0:07:53.\n",
            "  Batch   560  of  1,487. Loss: 0.027  Elapsed: 0:08:02.\n",
            "  Batch   600  of  1,487. Loss: 0.151  Elapsed: 0:08:11.\n",
            "  Batch   640  of  1,487. Loss: 0.864  Elapsed: 0:08:20.\n",
            "  Batch   680  of  1,487. Loss: 0.717  Elapsed: 0:08:29.\n",
            "  Batch   720  of  1,487. Loss: 0.314  Elapsed: 0:08:38.\n",
            "  Batch   760  of  1,487. Loss: 0.200  Elapsed: 0:08:47.\n",
            "  Batch   800  of  1,487. Loss: 0.536  Elapsed: 0:08:56.\n",
            "  Batch   840  of  1,487. Loss: 0.006  Elapsed: 0:09:05.\n",
            "  Batch   880  of  1,487. Loss: 0.245  Elapsed: 0:09:15.\n",
            "  Batch   920  of  1,487. Loss: 0.004  Elapsed: 0:09:24.\n",
            "  Batch   960  of  1,487. Loss: 0.063  Elapsed: 0:09:33.\n",
            "  Batch 1,000  of  1,487. Loss: 0.092  Elapsed: 0:09:42.\n",
            "  Batch 1,040  of  1,487. Loss: 0.002  Elapsed: 0:09:51.\n",
            "  Batch 1,080  of  1,487. Loss: 0.011  Elapsed: 0:10:00.\n",
            "  Batch 1,120  of  1,487. Loss: 0.243  Elapsed: 0:10:09.\n",
            "  Batch 1,160  of  1,487. Loss: 0.008  Elapsed: 0:10:18.\n",
            "  Batch 1,200  of  1,487. Loss: 0.059  Elapsed: 0:10:28.\n",
            "  Batch 1,240  of  1,487. Loss: 0.125  Elapsed: 0:10:37.\n",
            "  Batch 1,280  of  1,487. Loss: 0.004  Elapsed: 0:10:46.\n",
            "  Batch 1,320  of  1,487. Loss: 0.007  Elapsed: 0:10:55.\n",
            "  Batch 1,360  of  1,487. Loss: 0.022  Elapsed: 0:11:04.\n",
            "  Batch 1,400  of  1,487. Loss: 0.067  Elapsed: 0:11:13.\n",
            "  Batch 1,440  of  1,487. Loss: 0.241  Elapsed: 0:11:22.\n",
            "  Batch 1,480  of  1,487. Loss: 0.092  Elapsed: 0:11:31.\n",
            "Average training loss: 0.30\n",
            "  Accuracy: 0.8313\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of  1,487. Loss: 0.004  Elapsed: 0:11:55.\n",
            "  Batch    80  of  1,487. Loss: 0.017  Elapsed: 0:12:04.\n",
            "  Batch   120  of  1,487. Loss: 0.005  Elapsed: 0:12:13.\n",
            "  Batch   160  of  1,487. Loss: 0.004  Elapsed: 0:12:22.\n",
            "  Batch   200  of  1,487. Loss: 0.001  Elapsed: 0:12:31.\n",
            "  Batch   240  of  1,487. Loss: 0.003  Elapsed: 0:12:40.\n",
            "  Batch   280  of  1,487. Loss: 0.002  Elapsed: 0:12:49.\n",
            "  Batch   320  of  1,487. Loss: 0.002  Elapsed: 0:12:59.\n",
            "  Batch   360  of  1,487. Loss: 0.001  Elapsed: 0:13:08.\n",
            "  Batch   400  of  1,487. Loss: 0.001  Elapsed: 0:13:17.\n",
            "  Batch   440  of  1,487. Loss: 0.001  Elapsed: 0:13:26.\n",
            "  Batch   480  of  1,487. Loss: 1.086  Elapsed: 0:13:35.\n",
            "  Batch   520  of  1,487. Loss: 0.006  Elapsed: 0:13:44.\n",
            "  Batch   560  of  1,487. Loss: 0.010  Elapsed: 0:13:53.\n",
            "  Batch   600  of  1,487. Loss: 0.001  Elapsed: 0:14:02.\n",
            "  Batch   640  of  1,487. Loss: 0.001  Elapsed: 0:14:11.\n",
            "  Batch   680  of  1,487. Loss: 0.002  Elapsed: 0:14:20.\n",
            "  Batch   720  of  1,487. Loss: 0.001  Elapsed: 0:14:29.\n",
            "  Batch   760  of  1,487. Loss: 0.002  Elapsed: 0:14:38.\n",
            "  Batch   800  of  1,487. Loss: 0.001  Elapsed: 0:14:47.\n",
            "  Batch   840  of  1,487. Loss: 0.021  Elapsed: 0:14:56.\n",
            "  Batch   880  of  1,487. Loss: 0.002  Elapsed: 0:15:05.\n",
            "  Batch   920  of  1,487. Loss: 0.016  Elapsed: 0:15:14.\n",
            "  Batch   960  of  1,487. Loss: 0.968  Elapsed: 0:15:23.\n",
            "  Batch 1,000  of  1,487. Loss: 0.002  Elapsed: 0:15:32.\n",
            "  Batch 1,040  of  1,487. Loss: 0.001  Elapsed: 0:15:41.\n",
            "  Batch 1,080  of  1,487. Loss: 0.002  Elapsed: 0:15:50.\n",
            "  Batch 1,120  of  1,487. Loss: 0.758  Elapsed: 0:15:59.\n",
            "  Batch 1,160  of  1,487. Loss: 0.001  Elapsed: 0:16:08.\n",
            "  Batch 1,200  of  1,487. Loss: 0.001  Elapsed: 0:16:17.\n",
            "  Batch 1,240  of  1,487. Loss: 0.001  Elapsed: 0:16:26.\n",
            "  Batch 1,280  of  1,487. Loss: 0.001  Elapsed: 0:16:35.\n",
            "  Batch 1,320  of  1,487. Loss: 0.001  Elapsed: 0:16:44.\n",
            "  Batch 1,360  of  1,487. Loss: 0.001  Elapsed: 0:16:53.\n",
            "  Batch 1,400  of  1,487. Loss: 0.001  Elapsed: 0:17:02.\n",
            "  Batch 1,440  of  1,487. Loss: 0.001  Elapsed: 0:17:11.\n",
            "  Batch 1,480  of  1,487. Loss: 0.001  Elapsed: 0:17:20.\n",
            "Average training loss: 0.12\n",
            "  Accuracy: 0.8461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovzk0htRUlo7"
      },
      "source": [
        "predicted_labels = [] ; true_labels = []; logits_list = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  \n",
        "  input_ids = batch[0].to(device)\n",
        "  attention_masks = batch[1].to(device)\n",
        "  labels = batch[2]\n",
        "  \n",
        "  with torch.no_grad():        \n",
        "      outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "                  \n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  logits_list.append(logits)\n",
        "  \n",
        "  predictions = np.argmax(logits, axis=1).flatten()\n",
        "  labels = labels.numpy().flatten()\n",
        "\n",
        "  predicted_labels.extend( predictions )\n",
        "  true_labels.extend( labels )\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZnHVFaPUo8a",
        "outputId": "09334465-a62b-4d96-9678-43527a2732cf"
      },
      "source": [
        "# Parameters:\n",
        "#epochs = 2\n",
        "#lr = 3e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "#adam_epsilon = 1e-8\n",
        "#WARM_UP = 0\n",
        "#87 86 87\n",
        "from sklearn.metrics import classification_report \n",
        "print( classification_report(y_true=true_labels, y_pred=predicted_labels, zero_division=0) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       489\n",
            "           1       0.86      0.83      0.84       370\n",
            "           2       0.86      0.85      0.85       627\n",
            "\n",
            "    accuracy                           0.85      1486\n",
            "   macro avg       0.85      0.84      0.85      1486\n",
            "weighted avg       0.85      0.85      0.85      1486\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4hYocr4Ut8e",
        "outputId": "322fb25d-1b10-43b5-c9fd-84b3844a58d3"
      },
      "source": [
        "import os\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "output_dir = \"/content/drive/My Drive/model_bert_finetuned_1_2\"\n",
        "\n",
        "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "# If we have a distributed model, save only the encapsulated model\n",
        "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/model_bert_finetuned_1_2/vocab.txt',\n",
              " '/content/drive/My Drive/model_bert_finetuned_1_2/special_tokens_map.json',\n",
              " '/content/drive/My Drive/model_bert_finetuned_1_2/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHnZrkrNUx9N"
      },
      "source": [
        "# Step 2: Re-load the saved model and vocabulary\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et_Z-hlEaAOn"
      },
      "source": [
        "**Products**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU6AxwlWaGKb"
      },
      "source": [
        "import os\n",
        "\n",
        "TASK = 'SDS-P'\n",
        "\n",
        "for set_ in get_sets(TASK):\n",
        "  os.system('wget https://wothub-data.s3.amazonaws.com/Corpus/%s -nc'%set_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FnzfXuYbqAy"
      },
      "source": [
        "train_path, dev_path, test_path = get_sets(TASK)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U4fXApXbtAF"
      },
      "source": [
        "# Load data as pandas dataframes with two columns -- sentences and labels\n",
        "train_data = pd.read_csv(train_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "dev_data = pd.read_csv(dev_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "test_data = pd.read_csv(test_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfGmsi7wbyop"
      },
      "source": [
        "train_data = train_data[train_data.label != 'z_amb']\n",
        "dev_data = dev_data[dev_data.label != 'z_amb']\n",
        "test_data = test_data[test_data.label != 'z_amb']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt0FpNhJb1p9"
      },
      "source": [
        "# Convert to numpy arrays\n",
        "train_sentences, dev_sentences, test_sentences = [data.iloc[:,0] for data in (train_data, dev_data, test_data)]\n",
        "train_labels, dev_labels, test_labels = [data.iloc[:,1] for data in (train_data, dev_data, test_data)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ypzCLLbb4hP"
      },
      "source": [
        "# Check for errors in data labeling, removing nans\n",
        "def remove_nulls(sentences, labels):\n",
        "  lab = pd.Series(labels)\n",
        "  sen = pd.Series(sentences)\n",
        "  lab_nuls = pd.isnull(lab)\n",
        "  sen_nuls = pd.isnull(sen)\n",
        "  not_nuls = ~(lab_nuls | sen_nuls)\n",
        "  lab = lab.loc[not_nuls].to_numpy()\n",
        "  sen = sen.loc[not_nuls].to_numpy()\n",
        "  return [sen, lab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1gepY0Hb7kd"
      },
      "source": [
        "train_sentences, train_labels = remove_nulls(train_sentences, train_labels)\n",
        "dev_sentences, dev_labels = remove_nulls(dev_sentences, dev_labels)\n",
        "test_sentences, test_labels = remove_nulls(test_sentences, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG3Ohnz5b-wc",
        "outputId": "deacc1b1-3458-404b-92fb-88c5e047c973"
      },
      "source": [
        "np.unique(np.concatenate((train_labels, dev_labels, test_labels)), return_counts=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2]), array([3429, 1828,  695]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOpTCzBncDEM",
        "outputId": "0a80e463-8541-4189-d9ff-76feab2669dd"
      },
      "source": [
        "len(train_sentences)  * (1-0.875)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "594.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi6oGmcgtdJW",
        "outputId": "c8a9b365-32c9-4d1b-8c56-9e048074d1f2"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encoder for labels\n",
        "labelencoder = LabelEncoder()\n",
        "train_labels = labelencoder.fit_transform(train_labels)\n",
        "test_labels = labelencoder.transform(test_labels)\n",
        "dev_labels = labelencoder.transform(dev_labels)\n",
        "labelencoder.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['z_minus_m', 'z_plus_m', 'z_zero'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMb9hiNgcPGW"
      },
      "source": [
        "dev_sentences = np.append(dev_sentences, train_sentences[:594])\n",
        "dev_labels = np.append(dev_labels, train_labels[:594])\n",
        "train_sentences = train_sentences[594:]\n",
        "train_labels = train_labels[594:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1uQfEHPcSL0",
        "outputId": "fcdfcbfb-4ea5-447c-9bcd-4ab30419da66"
      },
      "source": [
        "print('Train data')\n",
        "print(len(train_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Dev data')\n",
        "print(len(dev_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Test data')\n",
        "print(len(test_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data\n",
            "0.6989247311827957\n",
            "Dev data\n",
            "0.1984206989247312\n",
            "Test data\n",
            "0.10265456989247312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh9vltUvcU7P"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMxBYMWGcYC8"
      },
      "source": [
        "dev_polemo_data = pd.DataFrame([dev_sentences,dev_labels]).T\n",
        "test_polemo_data = pd.DataFrame([test_sentences,test_labels]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBNL-zMecaqo"
      },
      "source": [
        "dev_polemo_data.to_csv('/content/drive/My Drive/dev_polemo_products_data_preprocessed.csv')\n",
        "test_polemo_data.to_csv('/content/drive/My Drive/test_polemo_products_data_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai4a98t1cgSF"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "X = train_sentences\n",
        "y = train_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  train_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  train_labels, test_labels = y[train_index], y[test_index]\n",
        "\n",
        "X = test_sentences\n",
        "y = test_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  dev_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  dev_labels, test_labels = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVip7fWgcjOJ"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encoder for labels\n",
        "labelencoder = LabelEncoder()\n",
        "train_labels = labelencoder.fit_transform(train_labels)\n",
        "test_labels = labelencoder.transform(test_labels)\n",
        "dev_labels = labelencoder.transform(dev_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx1meqPkcmdJ"
      },
      "source": [
        "# Remove long sentences.\n",
        "# TO-DO Possible cut?\n",
        "def remove_big(sentences, labels):\n",
        "  to_remove = []\n",
        "  for i, sent in enumerate(sentences):\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True) # TO-DO: add_special_tokens\n",
        "      if len(input_ids) > MAX_LEN:\n",
        "        to_remove.append(i)\n",
        "\n",
        "  sentences = np.delete(sentences, to_remove)\n",
        "  labels = np.delete(labels, to_remove) \n",
        "\n",
        "  print('{} samples removed.'.format(len(to_remove)))\n",
        "\n",
        "  return sentences, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55o7g2bwco7i"
      },
      "source": [
        "# Downloading tokenizer\n",
        "# From Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM7Y6J9gcrmG",
        "outputId": "aa7246d9-8c92-4c5d-a3e8-ec608bf8ab21"
      },
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "train_sentences, train_labels = remove_big(train_sentences, train_labels)\n",
        "test_sentences, test_labels = remove_big(test_sentences, test_labels)\n",
        "dev_sentences, dev_labels = remove_big(dev_sentences, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 samples removed.\n",
            "2 samples removed.\n",
            "4 samples removed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyziLQjtcuqz"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "# Create TensorDatasets for train/dev/test sets\n",
        "def tensor_dataset(sentences, labels):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in sentences:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                     \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = MAX_LEN,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hgDEhjgcxeT"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataset = tensor_dataset(train_sentences, train_labels)\n",
        "test_dataset = tensor_dataset(test_sentences, test_labels)\n",
        "dev_dataset = tensor_dataset(dev_sentences, dev_labels)\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "# Create the DataLoaders for train/dev/test sets.\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = BATCH_SIZE)\n",
        "validation_dataloader = DataLoader(dev_dataset, sampler = SequentialSampler(dev_dataset), batch_size = BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBuA01X8c0Ui",
        "outputId": "98e313f1-300e-409d-82ae-460e9c237a3c"
      },
      "source": [
        "# Load model with a sequence classification head\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dkleczek/bert-base-polish-uncased-v1\", # Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(60000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aTJBuRbc3ZX",
        "outputId": "872a8397-58dc-4abc-8525-527a2c22bb79"
      },
      "source": [
        "import time, datetime\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.optimization import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Takes a time in seconds and returns a string hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Parameters:\n",
        "epochs = 3\n",
        "#lr = 1e-3 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "lr = 3e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "adam_epsilon = 1e-8\n",
        "WARM_UP = 0\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = adam_epsilon)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = WARM_UP, num_training_steps = total_steps)\n",
        "\n",
        "train_loss_values = []\n",
        "dev_acc_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch_i in range(0, epochs):  \n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "  print('Training...')\n",
        "\n",
        "  # https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\n",
        "  # linie 168-183\n",
        "  epoch_train_loss = 0 # Cumulative loss\n",
        "  loss = 0 ;     batch_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Progress update every 40 batches.\n",
        "    if step % 40 == 0 and not step == 0:\n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)      \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}. Loss: {:.3f}  Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "    \n",
        "\n",
        "    batch_loss = 0\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)    \n",
        "\n",
        "    # clear any previously calculated gradients before backward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    epoch_train_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Update learning rate schedule\n",
        "\n",
        "  epoch_train_loss = epoch_train_loss / len(train_dataloader)          \n",
        "  train_loss_values.append(epoch_train_loss)\n",
        "  \n",
        "  print('Average training loss: {0:.2f}'.format(epoch_train_loss))\n",
        "\n",
        "  # Evaluation\n",
        "  total_eval_accuracy = 0\n",
        "  model.eval()\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    \n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2].to('cpu').numpy()\n",
        "                \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predictions = np.argmax(logits, axis=1).flatten()\n",
        "    total_eval_accuracy += flat_accuracy(logits, labels)\n",
        "\n",
        "  avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "  print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    415. Loss: 0.689  Elapsed: 0:00:09.\n",
            "  Batch    80  of    415. Loss: 0.295  Elapsed: 0:00:17.\n",
            "  Batch   120  of    415. Loss: 0.496  Elapsed: 0:00:26.\n",
            "  Batch   160  of    415. Loss: 0.963  Elapsed: 0:00:34.\n",
            "  Batch   200  of    415. Loss: 0.355  Elapsed: 0:00:43.\n",
            "  Batch   240  of    415. Loss: 0.100  Elapsed: 0:00:52.\n",
            "  Batch   280  of    415. Loss: 0.276  Elapsed: 0:01:01.\n",
            "  Batch   320  of    415. Loss: 0.787  Elapsed: 0:01:10.\n",
            "  Batch   360  of    415. Loss: 1.025  Elapsed: 0:01:19.\n",
            "  Batch   400  of    415. Loss: 0.539  Elapsed: 0:01:28.\n",
            "Average training loss: 0.59\n",
            "  Accuracy: 0.8389\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    415. Loss: 0.326  Elapsed: 0:01:44.\n",
            "  Batch    80  of    415. Loss: 0.516  Elapsed: 0:01:54.\n",
            "  Batch   120  of    415. Loss: 0.040  Elapsed: 0:02:03.\n",
            "  Batch   160  of    415. Loss: 0.018  Elapsed: 0:02:12.\n",
            "  Batch   200  of    415. Loss: 0.008  Elapsed: 0:02:21.\n",
            "  Batch   240  of    415. Loss: 0.131  Elapsed: 0:02:30.\n",
            "  Batch   280  of    415. Loss: 0.126  Elapsed: 0:02:39.\n",
            "  Batch   320  of    415. Loss: 0.007  Elapsed: 0:02:48.\n",
            "  Batch   360  of    415. Loss: 0.003  Elapsed: 0:02:57.\n",
            "  Batch   400  of    415. Loss: 0.028  Elapsed: 0:03:07.\n",
            "Average training loss: 0.27\n",
            "  Accuracy: 0.8125\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    415. Loss: 0.010  Elapsed: 0:03:23.\n",
            "  Batch    80  of    415. Loss: 0.102  Elapsed: 0:03:32.\n",
            "  Batch   120  of    415. Loss: 0.004  Elapsed: 0:03:41.\n",
            "  Batch   160  of    415. Loss: 0.014  Elapsed: 0:03:50.\n",
            "  Batch   200  of    415. Loss: 0.030  Elapsed: 0:03:59.\n",
            "  Batch   240  of    415. Loss: 0.005  Elapsed: 0:04:08.\n",
            "  Batch   280  of    415. Loss: 0.016  Elapsed: 0:04:17.\n",
            "  Batch   320  of    415. Loss: 0.002  Elapsed: 0:04:26.\n",
            "  Batch   360  of    415. Loss: 0.009  Elapsed: 0:04:35.\n",
            "  Batch   400  of    415. Loss: 0.001  Elapsed: 0:04:45.\n",
            "Average training loss: 0.10\n",
            "  Accuracy: 0.8582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWICkhBHc6vP"
      },
      "source": [
        "predicted_labels = [] ; true_labels = []; logits_list = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  \n",
        "  input_ids = batch[0].to(device)\n",
        "  attention_masks = batch[1].to(device)\n",
        "  labels = batch[2]\n",
        "  \n",
        "  with torch.no_grad():        \n",
        "      outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "                  \n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  logits_list.append(logits)\n",
        "  \n",
        "  predictions = np.argmax(logits, axis=1).flatten()\n",
        "  labels = labels.numpy().flatten()\n",
        "\n",
        "  predicted_labels.extend( predictions )\n",
        "  true_labels.extend( labels )\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXLg9zDQc9Zk",
        "outputId": "2862f6c4-05b4-4487-ca18-28d304d6b9ce"
      },
      "source": [
        "# Parameters:\n",
        "#epochs = 2\n",
        "#lr = 3e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "#adam_epsilon = 1e-8\n",
        "#WARM_UP = 0\n",
        "#87 86 87\n",
        "from sklearn.metrics import classification_report \n",
        "print( classification_report(y_true=true_labels, y_pred=predicted_labels, zero_division=0) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.91      0.89       238\n",
            "           1       0.83      0.83      0.83       127\n",
            "           2       0.69      0.51      0.59        49\n",
            "\n",
            "    accuracy                           0.84       414\n",
            "   macro avg       0.80      0.75      0.77       414\n",
            "weighted avg       0.84      0.84      0.84       414\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTqVJCfwc_xt",
        "outputId": "c4b3a56d-40ef-40bc-e9df-a6a18b5c7189"
      },
      "source": [
        "import os\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "output_dir = \"/content/drive/My Drive/model_bert_finetuned_1_3\"\n",
        "\n",
        "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "# If we have a distributed model, save only the encapsulated model\n",
        "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/model_bert_finetuned_1_3/vocab.txt',\n",
              " '/content/drive/My Drive/model_bert_finetuned_1_3/special_tokens_map.json',\n",
              " '/content/drive/My Drive/model_bert_finetuned_1_3/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvH875dBdCuZ"
      },
      "source": [
        "# Step 2: Re-load the saved model and vocabulary\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgUf0lmzlD6B"
      },
      "source": [
        "**Reviews**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujgn4VqwlGX_"
      },
      "source": [
        "import os\n",
        "\n",
        "TASK = 'SDS-R'\n",
        "\n",
        "for set_ in get_sets(TASK):\n",
        "  os.system('wget https://wothub-data.s3.amazonaws.com/Corpus/%s -nc'%set_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDRlK0UnlGuq"
      },
      "source": [
        "train_path, dev_path, test_path = get_sets(TASK)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI_V_o_MlHM2"
      },
      "source": [
        "# Load data as pandas dataframes with two columns -- sentences and labels\n",
        "train_data = pd.read_csv(train_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "dev_data = pd.read_csv(dev_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "test_data = pd.read_csv(test_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxE2aHO2lHcl"
      },
      "source": [
        "train_data = train_data[train_data.label != 'z_amb']\n",
        "dev_data = dev_data[dev_data.label != 'z_amb']\n",
        "test_data = test_data[test_data.label != 'z_amb']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EovF5AiElHtt"
      },
      "source": [
        "# Convert to numpy arrays\n",
        "train_sentences, dev_sentences, test_sentences = [data.iloc[:,0] for data in (train_data, dev_data, test_data)]\n",
        "train_labels, dev_labels, test_labels = [data.iloc[:,1] for data in (train_data, dev_data, test_data)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjeVNQ0QlH-x"
      },
      "source": [
        "# Check for errors in data labeling, removing nans\n",
        "def remove_nulls(sentences, labels):\n",
        "  lab = pd.Series(labels)\n",
        "  sen = pd.Series(sentences)\n",
        "  lab_nuls = pd.isnull(lab)\n",
        "  sen_nuls = pd.isnull(sen)\n",
        "  not_nuls = ~(lab_nuls | sen_nuls)\n",
        "  lab = lab.loc[not_nuls].to_numpy()\n",
        "  sen = sen.loc[not_nuls].to_numpy()\n",
        "  return [sen, lab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq6uPSuclfkY"
      },
      "source": [
        "train_sentences, train_labels = remove_nulls(train_sentences, train_labels)\n",
        "dev_sentences, dev_labels = remove_nulls(dev_sentences, dev_labels)\n",
        "test_sentences, test_labels = remove_nulls(test_sentences, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tTO3gH8liog",
        "outputId": "962e542e-2a2e-4315-a69f-0a413f149abd"
      },
      "source": [
        "np.unique(np.concatenate((train_labels, dev_labels, test_labels)), return_counts=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2]), array([460, 900, 225]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkmUueADllbL",
        "outputId": "c35df1d2-ed46-4e05-d6d4-3e76cb27fab4"
      },
      "source": [
        "len(train_sentences)  * (1-0.875)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osvvmicitqCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d7583e-fec6-4756-9a74-44a229cb7598"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encoder for labels\n",
        "labelencoder = LabelEncoder()\n",
        "train_labels = labelencoder.fit_transform(train_labels)\n",
        "test_labels = labelencoder.transform(test_labels)\n",
        "dev_labels = labelencoder.transform(dev_labels)\n",
        "labelencoder.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['z_minus_m', 'z_plus_m', 'z_zero'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0aD4sOVlovA"
      },
      "source": [
        "dev_sentences = np.append(dev_sentences, train_sentences[:160])\n",
        "dev_labels = np.append(dev_labels, train_labels[:160])\n",
        "train_sentences = train_sentences[160:]\n",
        "train_labels = train_labels[160:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48wTdLD3ltt6",
        "outputId": "f5b6f4c8-adb3-4aec-a40d-8419acc882e9"
      },
      "source": [
        "print('Train data')\n",
        "print(len(train_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Dev data')\n",
        "print(len(dev_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Test data')\n",
        "print(len(test_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data\n",
            "0.7041009463722397\n",
            "Dev data\n",
            "0.2056782334384858\n",
            "Test data\n",
            "0.09022082018927445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-xUgEUVlydV"
      },
      "source": [
        "dev_polemo_data = pd.DataFrame([dev_sentences,dev_labels]).T\n",
        "test_polemo_data = pd.DataFrame([test_sentences,test_labels]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhGnZHw3l1zA"
      },
      "source": [
        "dev_polemo_data.to_csv('/content/drive/My Drive/dev_polemo_reviews_data_preprocessed.csv')\n",
        "test_polemo_data.to_csv('/content/drive/My Drive/test_polemo_reviews_data_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmlzEm95l-Mu"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "X = train_sentences\n",
        "y = train_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  train_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  train_labels, test_labels = y[train_index], y[test_index]\n",
        "\n",
        "X = test_sentences\n",
        "y = test_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  dev_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  dev_labels, test_labels = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2j4si_vmArr"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Encoder for labels\n",
        "labelencoder = LabelEncoder()\n",
        "train_labels = labelencoder.fit_transform(train_labels)\n",
        "test_labels = labelencoder.transform(test_labels)\n",
        "dev_labels = labelencoder.transform(dev_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ecHtaTmEO-"
      },
      "source": [
        "# Remove long sentences.\n",
        "# TO-DO Possible cut?\n",
        "def remove_big(sentences, labels):\n",
        "  to_remove = []\n",
        "  for i, sent in enumerate(sentences):\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True) # TO-DO: add_special_tokens\n",
        "      if len(input_ids) > MAX_LEN:\n",
        "        to_remove.append(i)\n",
        "\n",
        "  sentences = np.delete(sentences, to_remove)\n",
        "  labels = np.delete(labels, to_remove) \n",
        "\n",
        "  print('{} samples removed.'.format(len(to_remove)))\n",
        "\n",
        "  return sentences, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TCY5obOmHsu"
      },
      "source": [
        "# Downloading tokenizer\n",
        "# From Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98RS_0Z5mK6y",
        "outputId": "a66f4bd9-4ffe-4b86-9b19-d7fa78d95f49"
      },
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "train_sentences, train_labels = remove_big(train_sentences, train_labels)\n",
        "test_sentences, test_labels = remove_big(test_sentences, test_labels)\n",
        "dev_sentences, dev_labels = remove_big(dev_sentences, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 samples removed.\n",
            "0 samples removed.\n",
            "0 samples removed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOcHW-FRmTQB"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "# Create TensorDatasets for train/dev/test sets\n",
        "def tensor_dataset(sentences, labels):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in sentences:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                     \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = MAX_LEN,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm_oBSDTmV3Z"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataset = tensor_dataset(train_sentences, train_labels)\n",
        "test_dataset = tensor_dataset(test_sentences, test_labels)\n",
        "dev_dataset = tensor_dataset(dev_sentences, dev_labels)\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "# Create the DataLoaders for train/dev/test sets.\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = BATCH_SIZE)\n",
        "validation_dataloader = DataLoader(dev_dataset, sampler = SequentialSampler(dev_dataset), batch_size = BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktlK7sTBmYPy",
        "outputId": "9fe39226-773b-4c26-d8e1-48d1578d876a"
      },
      "source": [
        "# Load model with a sequence classification head\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dkleczek/bert-base-polish-uncased-v1\", # Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(60000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yunT8RVSmbS1",
        "outputId": "cf7e353c-f8b2-4473-8f49-9155539adeeb"
      },
      "source": [
        "import time, datetime\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.optimization import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Takes a time in seconds and returns a string hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Parameters:\n",
        "epochs = 3\n",
        "#lr = 1e-3 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "lr = 3e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "adam_epsilon = 1e-8\n",
        "WARM_UP = 0\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = adam_epsilon)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = WARM_UP, num_training_steps = total_steps)\n",
        "\n",
        "train_loss_values = []\n",
        "dev_acc_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch_i in range(0, epochs):  \n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "  print('Training...')\n",
        "\n",
        "  # https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\n",
        "  # linie 168-183\n",
        "  epoch_train_loss = 0 # Cumulative loss\n",
        "  loss = 0 ;     batch_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Progress update every 40 batches.\n",
        "    if step % 40 == 0 and not step == 0:\n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)      \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}. Loss: {:.3f}  Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "    \n",
        "\n",
        "    batch_loss = 0\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)    \n",
        "\n",
        "    # clear any previously calculated gradients before backward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    epoch_train_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Update learning rate schedule\n",
        "\n",
        "  epoch_train_loss = epoch_train_loss / len(train_dataloader)          \n",
        "  train_loss_values.append(epoch_train_loss)\n",
        "  \n",
        "  print('Average training loss: {0:.2f}'.format(epoch_train_loss))\n",
        "\n",
        "  # Evaluation\n",
        "  total_eval_accuracy = 0\n",
        "  model.eval()\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    \n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2].to('cpu').numpy()\n",
        "                \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predictions = np.argmax(logits, axis=1).flatten()\n",
        "    total_eval_accuracy += flat_accuracy(logits, labels)\n",
        "\n",
        "  avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "  print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    112. Loss: 0.836  Elapsed: 0:00:09.\n",
            "  Batch    80  of    112. Loss: 0.699  Elapsed: 0:00:18.\n",
            "Average training loss: 0.77\n",
            "  Accuracy: 0.7411\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    112. Loss: 0.129  Elapsed: 0:00:34.\n",
            "  Batch    80  of    112. Loss: 0.936  Elapsed: 0:00:43.\n",
            "Average training loss: 0.31\n",
            "  Accuracy: 0.7946\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    112. Loss: 0.073  Elapsed: 0:01:00.\n",
            "  Batch    80  of    112. Loss: 0.005  Elapsed: 0:01:10.\n",
            "Average training loss: 0.10\n",
            "  Accuracy: 0.8036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6awB6DLmex6"
      },
      "source": [
        "predicted_labels = [] ; true_labels = []; logits_list = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  \n",
        "  input_ids = batch[0].to(device)\n",
        "  attention_masks = batch[1].to(device)\n",
        "  labels = batch[2]\n",
        "  \n",
        "  with torch.no_grad():        \n",
        "      outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "                  \n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  logits_list.append(logits)\n",
        "  \n",
        "  predictions = np.argmax(logits, axis=1).flatten()\n",
        "  labels = labels.numpy().flatten()\n",
        "\n",
        "  predicted_labels.extend( predictions )\n",
        "  true_labels.extend( labels )\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQbkMMyTmh6E",
        "outputId": "8c5549eb-2f9e-4058-d90b-7b0e2c354944"
      },
      "source": [
        "# Parameters:\n",
        "#epochs = 2\n",
        "#lr = 3e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "#adam_epsilon = 1e-8\n",
        "#WARM_UP = 0\n",
        "#87 86 87\n",
        "from sklearn.metrics import classification_report \n",
        "print( classification_report(y_true=true_labels, y_pred=predicted_labels, zero_division=0) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84        33\n",
            "           1       0.86      0.86      0.86        63\n",
            "           2       0.62      0.50      0.55        16\n",
            "\n",
            "    accuracy                           0.81       112\n",
            "   macro avg       0.76      0.75      0.75       112\n",
            "weighted avg       0.81      0.81      0.81       112\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4xceB8ImkS1",
        "outputId": "92aa3c1c-efdd-4c09-f085-2a0fdc395610"
      },
      "source": [
        "import os\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "output_dir = \"/content/drive/My Drive/model_bert_finetuned_1_4\"\n",
        "\n",
        "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "# If we have a distributed model, save only the encapsulated model\n",
        "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/model_bert_finetuned_1_4/vocab.txt',\n",
              " '/content/drive/My Drive/model_bert_finetuned_1_4/special_tokens_map.json',\n",
              " '/content/drive/My Drive/model_bert_finetuned_1_4/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpTWSr8jmmZJ"
      },
      "source": [
        "# Step 2: Re-load the saved model and vocabulary\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
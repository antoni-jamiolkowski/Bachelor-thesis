{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "filmweb_base_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkRdSCnY3Vul"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmAh_Ou93tDQ"
      },
      "source": [
        "data = pd.read_excel(\"/content/recenzje_z_sentymentem.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT6HdmZp4SJ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "d7afcc02-5847-4491-f874-7615d6c811cc"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recenzja</th>\n",
              "      <th>liczba gwiazdek</th>\n",
              "      <th>sentyment</th>\n",
              "      <th>adres URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Historia cudna, główni bohaterowie francuskiej...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Nietykalni-2011-58...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To dobry film i daje dużo do myślenia. W sumie...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Dobrze+się+kłamie+...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dobry film.Jak na komedię to moim zdaniem jest...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Dobrze+się+kłamie+...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nigdy nie oceniałem filmu na filmwebie, jakoś ...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Narodziny+gwiazdy-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Film bardzo mi się spodobał, jednak zastanawia...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Eksperyment-2010-4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            recenzja  ...                                          adres URL\n",
              "0  Historia cudna, główni bohaterowie francuskiej...  ...  https://www.filmweb.pl/film/Nietykalni-2011-58...\n",
              "1  To dobry film i daje dużo do myślenia. W sumie...  ...  https://www.filmweb.pl/film/Dobrze+się+kłamie+...\n",
              "2  Dobry film.Jak na komedię to moim zdaniem jest...  ...  https://www.filmweb.pl/film/Dobrze+się+kłamie+...\n",
              "3  Nigdy nie oceniałem filmu na filmwebie, jakoś ...  ...  https://www.filmweb.pl/film/Narodziny+gwiazdy-...\n",
              "4  Film bardzo mi się spodobał, jednak zastanawia...  ...  https://www.filmweb.pl/film/Eksperyment-2010-4...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSvKaah44T6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e9772e-4f21-4cbf-a7ca-e57abf8f75aa"
      },
      "source": [
        "data['sentyment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    400\n",
              "-1    300\n",
              " 0    300\n",
              "Name: sentyment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTGTt0go4fqH"
      },
      "source": [
        "def rescale_sentiment(x):\n",
        "  if x == 1:\n",
        "    return x\n",
        "  elif x == 0:\n",
        "    return 2\n",
        "  elif x == -1:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBdFlc7v45oS"
      },
      "source": [
        "data['sentyment'] = data['sentyment'].map(rescale_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrtWdK4T5Dai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d03d06-9aad-454e-c029-2c2acb496f0f"
      },
      "source": [
        "data['sentyment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    400\n",
              "2    300\n",
              "0    300\n",
              "Name: sentyment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I59qDNzp5Fzu"
      },
      "source": [
        "with open('/content/stopwords.txt', \"r\") as f:\n",
        "  STOP_WORDS = set([line.rstrip(\"\\n\") for line in f])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_1mS7ky5b2G"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess(intext: str):\n",
        "  text = re.sub(r'\\W+', ' ', intext.lower()).split()\n",
        "  text = \" \".join([word for word in text if word not in STOP_WORDS])\n",
        "\n",
        "  return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCuMuqTS5hcl"
      },
      "source": [
        "data['recenzja'] = data['recenzja'].map(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq-SBvuS5ndZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "5d309751-8bd9-4336-f9e0-04d001e98f3d"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recenzja</th>\n",
              "      <th>liczba gwiazdek</th>\n",
              "      <th>sentyment</th>\n",
              "      <th>adres URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>historia cudna główni bohaterowie francuskiej ...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Nietykalni-2011-58...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dobry film daje myślenia sumie ciężki widza ni...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Dobrze+się+kłamie+...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dobry film komedię zdaniem zabawny film ciekaw...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Dobrze+się+kłamie+...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>oceniałem filmu filmwebie kręciło założyłem ko...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Narodziny+gwiazdy-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>film spodobał zastanawia kwestii pierwsze stra...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.filmweb.pl/film/Eksperyment-2010-4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            recenzja  ...                                          adres URL\n",
              "0  historia cudna główni bohaterowie francuskiej ...  ...  https://www.filmweb.pl/film/Nietykalni-2011-58...\n",
              "1  dobry film daje myślenia sumie ciężki widza ni...  ...  https://www.filmweb.pl/film/Dobrze+się+kłamie+...\n",
              "2  dobry film komedię zdaniem zabawny film ciekaw...  ...  https://www.filmweb.pl/film/Dobrze+się+kłamie+...\n",
              "3  oceniałem filmu filmwebie kręciło założyłem ko...  ...  https://www.filmweb.pl/film/Narodziny+gwiazdy-...\n",
              "4  film spodobał zastanawia kwestii pierwsze stra...  ...  https://www.filmweb.pl/film/Eksperyment-2010-4...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdlyA-Q06S3z"
      },
      "source": [
        "film_data = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syWNOFK935x2"
      },
      "source": [
        "film_data.to_csv('/content/drive/My Drive/filmweb_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uok5i7Qs5tj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a38d654-5564-4a46-cfab-e23984ee9c48"
      },
      "source": [
        "!pip install transformers==2.8.0 -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 573kB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 16.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 31.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 58.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 54.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.9MB 55.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: botocore 1.19.25 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp29kcct5ySF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzd74kjZ50dJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5e7a20-f807-4e36-c113-dcc18656a904"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#model.cuda()\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2aAi5Lj4B7n"
      },
      "source": [
        "film_data = pd.read_csv('/content/drive/My Drive/filmweb_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igo1T1VT52Vk"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "X = film_data.recenzja.to_numpy()\n",
        "y = film_data.sentyment.to_numpy()\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  train_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  train_labels, test_labels = y[train_index], y[test_index]\n",
        "\n",
        "X = test_sentences\n",
        "y = test_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.33, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  dev_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  dev_labels, test_labels = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1z2LR_N6B8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7bda15-a06a-40d1-b7dd-acb7a586055d"
      },
      "source": [
        "print('Train data')\n",
        "print(len(train_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Dev data')\n",
        "print(len(dev_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Test data')\n",
        "print(len(test_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data\n",
            "0.7\n",
            "Dev data\n",
            "0.201\n",
            "Test data\n",
            "0.099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBYOr09g-5mq"
      },
      "source": [
        "dev_filmweb_data = pd.DataFrame([dev_sentences,dev_labels]).T\n",
        "test_filmweb_data = pd.DataFrame([test_sentences,test_labels]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNpjQmE8_AfT"
      },
      "source": [
        "dev_filmweb_data.to_csv('/content/drive/My Drive/dev_filmweb_data_preprocessed.csv')\n",
        "test_filmweb_data.to_csv('/content/drive/My Drive/test_filmweb_data_preprocessed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Yb7Jua6eHG"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "X = train_sentences\n",
        "y = train_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  train_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  train_labels, test_labels = y[train_index], y[test_index]\n",
        "\n",
        "X = test_sentences\n",
        "y = test_labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "  dev_sentences, test_sentences = X[train_index], X[test_index]\n",
        "  dev_labels, test_labels = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AboWQT2s6hxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac6d1d8-26aa-4ecb-bd41-5c28a62e7243"
      },
      "source": [
        "print('Train data')\n",
        "print(len(train_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Dev data')\n",
        "print(len(dev_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))\n",
        "print('Test data')\n",
        "print(len(test_sentences) / ( len(train_sentences) + len(dev_sentences) + len(test_sentences) ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data\n",
            "0.8\n",
            "Dev data\n",
            "0.1\n",
            "Test data\n",
            "0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzediaYd6mQK"
      },
      "source": [
        "# Remove long sentences.\n",
        "# TO-DO Possible cut?\n",
        "def remove_big(sentences, labels):\n",
        "  to_remove = []\n",
        "  for i, sent in enumerate(sentences):\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True) # TO-DO: add_special_tokens\n",
        "      if len(input_ids) > MAX_LEN:\n",
        "        to_remove.append(i)\n",
        "\n",
        "  sentences = np.delete(sentences, to_remove)\n",
        "  labels = np.delete(labels, to_remove) \n",
        "\n",
        "  print('{} samples removed.'.format(len(to_remove)))\n",
        "\n",
        "  return sentences, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eejQt-gQ6p-R"
      },
      "source": [
        "# Downloading tokenizer\n",
        "# From Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aah8bA36ri_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ea4e97-0f46-4f05-bf07-5a3b141f1b7f"
      },
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "train_sentences, train_labels = remove_big(train_sentences, train_labels)\n",
        "test_sentences, test_labels = remove_big(test_sentences, test_labels)\n",
        "dev_sentences, dev_labels = remove_big(dev_sentences, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 samples removed.\n",
            "1 samples removed.\n",
            "0 samples removed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T37Rdx_6tVy"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "# Create TensorDatasets for train/dev/test sets\n",
        "def tensor_dataset(sentences, labels):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in sentences:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                     \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = MAX_LEN,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz3ohY896vQ5"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataset = tensor_dataset(train_sentences, train_labels)\n",
        "test_dataset = tensor_dataset(test_sentences, test_labels)\n",
        "dev_dataset = tensor_dataset(dev_sentences, dev_labels)\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "# Create the DataLoaders for train/dev/test sets.\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = BATCH_SIZE)\n",
        "validation_dataloader = DataLoader(dev_dataset, sampler = SequentialSampler(dev_dataset), batch_size = BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSMGe7aU6yux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45354f1b-140a-42a8-ae16-63be36e603fe"
      },
      "source": [
        "# Load model with a sequence classification head\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dkleczek/bert-base-polish-uncased-v1\", # Polbert - Polish BERT by Darek Kłeczek: https://github.com/kldarek/polbert\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(60000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFkg2pWZ7KSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b76b7f1-cdf4-4c8f-daab-d56a12b056f6"
      },
      "source": [
        "import time, datetime\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.optimization import AdamW\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# Takes a time in seconds and returns a string hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Parameters:\n",
        "epochs = 3\n",
        "#lr = 3e-3 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "lr = 5e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "adam_epsilon = 1e-10\n",
        "WARM_UP = 0\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = adam_epsilon)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = WARM_UP, num_training_steps = total_steps)\n",
        "\n",
        "train_loss_values = []\n",
        "dev_acc_values = []\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch_i in range(0, epochs):  \n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "  print('Training...')\n",
        "\n",
        "  # https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\n",
        "  # linie 168-183\n",
        "  epoch_train_loss = 0 # Cumulative loss\n",
        "  loss = 0 ;     batch_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # Progress update every 40 batches.\n",
        "    if step % 40 == 0 and not step == 0:\n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)      \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}. Loss: {:.3f}  Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "    \n",
        "    batch_loss = 0\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)    \n",
        "\n",
        "    # clear any previously calculated gradients before backward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    epoch_train_loss += loss.item()\n",
        "    batch_loss += loss.item()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Update learning rate schedule\n",
        "\n",
        "  epoch_train_loss = epoch_train_loss / len(train_dataloader)          \n",
        "  train_loss_values.append(epoch_train_loss)\n",
        "  \n",
        "  print('Average training loss: {0:.2f}'.format(epoch_train_loss))\n",
        "\n",
        "  # Evaluation\n",
        "  total_eval_accuracy = 0\n",
        "  model.eval()\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    \n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_masks = batch[1].to(device)\n",
        "    labels = batch[2].to('cpu').numpy()\n",
        "                \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predictions = np.argmax(logits, axis=1).flatten()\n",
        "    total_eval_accuracy += flat_accuracy(logits, labels)\n",
        "\n",
        "  avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "  print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of     70. Loss: 1.121  Elapsed: 0:00:08.\n",
            "Average training loss: 0.96\n",
            "  Accuracy: 0.5787\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of     70. Loss: 0.181  Elapsed: 0:00:23.\n",
            "Average training loss: 0.45\n",
            "  Accuracy: 0.5880\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of     70. Loss: 0.520  Elapsed: 0:00:38.\n",
            "Average training loss: 0.15\n",
            "  Accuracy: 0.6481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ4SO8-7RF2"
      },
      "source": [
        "predicted_labels = [] ; true_labels = []; logits_list = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "  \n",
        "  input_ids = batch[0].to(device)\n",
        "  attention_masks = batch[1].to(device)\n",
        "  labels = batch[2]\n",
        "  \n",
        "  with torch.no_grad():        \n",
        "      outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "                  \n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  logits_list.append(logits)\n",
        "  \n",
        "  predictions = np.argmax(logits, axis=1).flatten()\n",
        "  labels = labels.numpy().flatten()\n",
        "\n",
        "  predicted_labels.extend( predictions )\n",
        "  true_labels.extend( labels )\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nHMYnVc7T9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed4d5f3-d93f-43ca-8964-969c8683cb2b"
      },
      "source": [
        "# Parameters:\n",
        "#epochs = 3\n",
        "#lr = 2e-5 # Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "#adam_epsilon = 1e-8\n",
        "#WARM_UP = 0.1\n",
        "#0.72-73\n",
        "from sklearn.metrics import classification_report \n",
        "print( classification_report(y_true=true_labels, y_pred=predicted_labels, zero_division=0) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.75      0.65        20\n",
            "           1       0.81      0.79      0.80        28\n",
            "           2       0.56      0.43      0.49        21\n",
            "\n",
            "    accuracy                           0.67        69\n",
            "   macro avg       0.65      0.65      0.65        69\n",
            "weighted avg       0.67      0.67      0.66        69\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLG6t-Qg7ZBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c196a6-5758-481f-eeb0-44e375eb4c8c"
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpQ4_cDrFzn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c3a394-e1b0-4ae7-8012-7fbf9f11f046"
      },
      "source": [
        "import os\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "output_dir = \"/content/drive/My Drive/model_bert_finetuned_3\"\n",
        "\n",
        "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "# If we have a distributed model, save only the encapsulated model\n",
        "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/model_bert_finetuned_3/vocab.txt',\n",
              " '/content/drive/My Drive/model_bert_finetuned_3/special_tokens_map.json',\n",
              " '/content/drive/My Drive/model_bert_finetuned_3/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV-gxFs_GCcP"
      },
      "source": [
        "# Step 2: Re-load the saved model and vocabulary\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keAArZNeGEyk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}